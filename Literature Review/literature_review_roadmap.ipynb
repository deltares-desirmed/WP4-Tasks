{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cd048d",
   "metadata": {},
   "source": [
    "RoadMap- first export search database in RIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ad268",
   "metadata": {},
   "source": [
    "RIS to xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ba680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def blank_row(length):\n",
    "    \"\"\"Create and return a blank row with the given number of columns.\"\"\"\n",
    "    return [None] * length\n",
    "\n",
    "def convert_ris_to_excel(ris_path):\n",
    "    \"\"\"Convert a single RIS file to XLSX, combining repeated fields like AU, KW.\"\"\"\n",
    "    xlsx_path = ris_path.replace(\".ris\", \".xlsx\")\n",
    "\n",
    "    # Read and flatten RIS text\n",
    "    with open(ris_path, 'r', encoding='utf-8') as ris_file:\n",
    "        ris_text = ris_file.read().replace(\"\\n\", \" \")\n",
    "\n",
    "    # Extract tags and values using regex\n",
    "    regex = re.compile(r'([A-Z][A-Z0-9])  - (.*?)(?=([A-Z][A-Z0-9])  - |$)')\n",
    "    matches = re.findall(regex, ris_text)\n",
    "\n",
    "    if not matches:\n",
    "        print(f\"No valid RIS data found in {os.path.basename(ris_path)}.\")\n",
    "        return\n",
    "\n",
    "    # Track order of first occurrence of each tag\n",
    "    headers = []\n",
    "    seen = set()\n",
    "    for match in matches:\n",
    "        tag = match[0]\n",
    "        if tag not in seen:\n",
    "            headers.append(tag)\n",
    "            seen.add(tag)\n",
    "\n",
    "    column_map = {header: index for index, header in enumerate(headers)}\n",
    "    data = []\n",
    "    row = blank_row(len(headers))\n",
    "\n",
    "    for match in matches:\n",
    "        ris_id, ris_data = match[0], match[1]\n",
    "        idx = column_map[ris_id]\n",
    "\n",
    "        # Append multiple values with separator\n",
    "        if row[idx]:\n",
    "            row[idx] += \"; \" + ris_data\n",
    "        else:\n",
    "            row[idx] = ris_data\n",
    "\n",
    "        # End of one RIS entry\n",
    "        if ris_id == \"ER\":\n",
    "            data.append(row)\n",
    "            row = blank_row(len(headers))\n",
    "\n",
    "    if any(row):\n",
    "        data.append(row)\n",
    "\n",
    "    # Write to Excel\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df.to_excel(xlsx_path, index=False, engine='openpyxl')\n",
    "    print(f\"Converted: {os.path.basename(ris_path)} → {os.path.basename(xlsx_path)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Batch convert all RIS files in a folder to Excel.\"\"\"\n",
    "    folder_path = r\"C:\\Users\\xxxxxxx\\Data\"\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(\"The specified directory does not exist.\")\n",
    "        return\n",
    "\n",
    "    ris_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".ris\")]\n",
    "\n",
    "    if not ris_files:\n",
    "        print(\"No .ris files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    for filename in ris_files:\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        convert_ris_to_excel(full_path)\n",
    "\n",
    "    print(\"All RIS files have been successfully converted to Excel.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting RIS to Excel conversion...\")\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "    finally:\n",
    "        input(\"Press <Enter> to close the program.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4966c0",
   "metadata": {},
   "source": [
    "xlsl to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeae015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def convert_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    This function attempts to read an Excel file, repair it if needed, and convert it to CSV.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Error: File not found.\")\n",
    "        return\n",
    "\n",
    "    # Try opening as an Excel file\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, engine='openpyxl')  # Open with openpyxl\n",
    "        new_file_path = file_path.replace(\".xlsx\", \"_converted.xlsx\")\n",
    "        \n",
    "        # Save a repaired copy\n",
    "        df.to_excel(new_file_path, index=False)\n",
    "        print(f\"Successfully repaired and saved as: {new_file_path}\")\n",
    "\n",
    "        # Also save as CSV\n",
    "        csv_file_path = file_path.replace(\".xlsx\", \"_converted.csv\")\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Converted to CSV: {csv_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the Excel file: {e}\")\n",
    "\n",
    "# Specify the file path (Ensure it is properly formatted for Windows)\n",
    "file_path = r\"C:\\Users\\xxxxxx\\Data\\new2.csv\"\n",
    "\n",
    "# Run the function\n",
    "convert_excel_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076d40f",
   "metadata": {},
   "source": [
    "Merge xlsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b269e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Folder path and output\n",
    "folder_path = r\"C:\\Users\\xxxxxxxx\\Data\"\n",
    "output_path = os.path.join(folder_path, \"Merged_Correctly_Formatted.xlsx\")\n",
    "\n",
    "# 2. Collect all .xls or .xlsx files\n",
    "xls_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.xls', '.xlsx'))]\n",
    "\n",
    "if not xls_files:\n",
    "    print(\" No Excel files (.xls/.xlsx) found.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Get structure from first file\n",
    "first_df = pd.read_excel(os.path.join(folder_path, xls_files[0]), dtype=str)\n",
    "columns = first_df.columns.tolist()\n",
    "merged = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 4. Merge loop with safety checks\n",
    "for file in xls_files:\n",
    "    full_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        df = pd.read_excel(full_path, dtype=str)\n",
    "        df = df[df.columns.intersection(columns)]  # Keep only known columns\n",
    "\n",
    "        if df.shape[1] == 0:\n",
    "            print(f\" Skipped {file}: No matching columns.\")\n",
    "            continue\n",
    "\n",
    "        df = df[~df.iloc[:, 0].astype(str).str.contains(columns[0], na=False)]  # Remove header-as-row\n",
    "        df = df.reindex(columns=columns)  # Reorder columns\n",
    "        merged = pd.concat([merged, df], ignore_index=True)\n",
    "        print(f\" Merged: {file} ({df.shape[0]} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to merge {file}: {e}\")\n",
    "\n",
    "# 5. Save merged output\n",
    "merged.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "print(f\"\\n Final merged file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14703147",
   "metadata": {},
   "source": [
    "sometimes you have to merge to retain or extract the columns you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17772699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List of CSV file paths (replace with actual file paths when running)\n",
    "file_paths = [\n",
    "    # Example placeholders — replace with actual file paths\n",
    "    r\"<path_to>/Action1.csv\",\n",
    "    r\"<path_to>/Agency1.csv\",\n",
    "    r\"<path_to>/Culture1.csv\",\n",
    "    # Add all remaining files here...\n",
    "]\n",
    "\n",
    "# Define expected column structure based on RIS tags\n",
    "expected_columns = ['AB', 'AU', 'DA', 'DO', 'EP', 'ER', 'IS', 'JO', 'KW',\n",
    "                    'PY', 'SN', 'SP', 'T1', 'T2', 'TY', 'UR', 'VL']\n",
    "\n",
    "# List to hold cleaned dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each file and process\n",
    "for file in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8-sig', sep=',', engine='python')\n",
    "\n",
    "        # Skip if dataframe is empty or has too few columns\n",
    "        if df.empty or len(df.columns) < 3:\n",
    "            continue\n",
    "\n",
    "        # Keep only expected columns and enforce consistent order\n",
    "        df = df.loc[:, df.columns.intersection(expected_columns)]\n",
    "        df = df.reindex(columns=expected_columns)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Merge all cleaned dataframes\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df.dropna(how='all', inplace=True)\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "output_path = r\"<path_to_output>/MergedData_Cleaned.csv\"\n",
    "merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Cleaned and merged data saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb824fe5",
   "metadata": {},
   "source": [
    "filter large dataset again with stricter keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc224b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths (update with actual paths when running)\n",
    "input_path = r\"<path_to>/Combined_entries_with_doi_exported.csv\"\n",
    "output_path = r\"<path_to>/filtered_updated_citations.xlsx\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Define keywords and semantically related terms\n",
    "search_terms = [\n",
    "    \"decision-making\", \"decision making\", \"governance\", \n",
    "    \"urban planning\", \"spatial planning\", \n",
    "    \"policy formulation\", \"policy design\", \"policy development\", \n",
    "    \"artificial intelligence\", \"machine learning\", \"AI\", \"algorithmic\"\n",
    "]\n",
    "\n",
    "# Optional: limit search to specific fields (e.g., abstract, title, keywords)\n",
    "search_fields = ['AB', 'T1', 'KW']  # Adjust these column names to match your dataset\n",
    "\n",
    "# Filter rows where any field contains any of the search terms (case-insensitive)\n",
    "filtered_df = df[df[search_fields].astype(str).apply(\n",
    "    lambda x: x.str.contains('|'.join(search_terms), case=False).any(), axis=1)]\n",
    "\n",
    "# Save results to Excel\n",
    "filtered_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769bf30",
   "metadata": {},
   "source": [
    "you csn use A combination of keyword search and NLP - BERT(Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# === 4. Load model ===\n",
    "print(\" Loading transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0120f",
   "metadata": {},
   "source": [
    "The rest of the anlysis depends on what questions you want the database to answer....you can combine a quantititve with Qualitative synthesis...... See an example of such paper [Artfifical intelligent for urban planning governance](https://doi.org/10.1016/j.landurbplan.2025.105337)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
